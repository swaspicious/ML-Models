{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQRrhDxbnx92fdeUYtSWA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swaspicious/ML-Models/blob/main/Building_Lasso_Regression_model_from_scratch_in_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***LASSO - Least Absolute Shrinkage and Selection Operator***"
      ],
      "metadata": {
        "id": "T_LCea2iEQRr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg6Yy0PtDqIE"
      },
      "outputs": [],
      "source": [
        "# importing the dependencies:\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression Class:**"
      ],
      "metadata": {
        "id": "TApHSPqcFtJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a class (a template or a blueprint) for lasso regression:\n",
        "\n",
        "class lassoregression():\n",
        "\n",
        "  # initiating the hyperparamteres:\n",
        "  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "    self.lambda_parameter = lambda_parameter\n",
        "\n",
        "\n",
        "  # fitting the dataset to lasso regression model:\n",
        "  def fit(self, x, y):\n",
        "\n",
        "    self.m , self.n = x.shape  # m -> no. of datapoints in the dataset (no. of rows) , n -> no. of input features in the dataset (no. of columns except target feature)\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "    # implementing gradient descent algorithm for optimization:\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_weights()\n",
        "\n",
        "\n",
        "  # function for updating the model parameters (weight and bias value):\n",
        "  def update_weights(self):\n",
        "\n",
        "    # linear equation of the model (since lasso regression is based on linear regression):\n",
        "    y_pred = self.predict(self.x)\n",
        "\n",
        "    # gradients (dw , db):  dw  is basically (dJ/dw) and db is basically (dJ/db), where J is the cost function.\n",
        "    dw = np.zeros(self.n) # dw is an array of all gradients (one per feature)\n",
        "    db = 0\n",
        "\n",
        "    for i in range(self.n):  # this loop is for considering all values of x[i] -> for each input feature.\n",
        "\n",
        "      if self.w[i] > 0:\n",
        "        dw[i] = (-(2*(self.x[:,i]).dot(self.y - y_pred)) + self.lambda_parameter) / self.m  # x[: , i] represents that after every iteration the value of i will change and x[i] will be initiated in that place. This is for the case when there are multiple input features, so the x[i] value is initiated properly each time (for i=1,2,3...n).\n",
        "\n",
        "      else:\n",
        "        dw[i] = (-(2*(self.x[:,i]).dot(self.y - y_pred)) - self.lambda_parameter) / self.m\n",
        "\n",
        "\n",
        "    db = -2 * np.sum(self.y - y_pred) / self.m  # both 'y' and 'y_pred' are numpy arrays, so they hold all the 'm' no. of values in them. so element-wise subtraction happens first and then the sum of resultant array is taken.\n",
        "\n",
        "    # updating the weights and bias using gradient descent:\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "\n",
        "  # predicting the target variable:\n",
        "  def predict(self, x):\n",
        "\n",
        "    return x.dot(self.w) + self.b\n"
      ],
      "metadata": {
        "id": "4q6sU9kAFr5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model = lassoregression()  ->  loading an instance of the class lassoregression to the variable 'model'.\n",
        "\n",
        "So model.fit(xtrain, train) will be interpreted as self.(x,y) inside the class."
      ],
      "metadata": {
        "id": "VbJp6DW5HZ7C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-nzVZLEG8rX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}